{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from langchain.messages import AnyMessage\n",
    "from typing_extensions import TypedDict, List, Annotated, Optional, Literal\n",
    "from langchain.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_tavily import TavilySearch\n",
    "import os\n",
    "import operator\n",
    "from pydantic import BaseModel, Field\n",
    "import heapq\n",
    "\n",
    "from config import config_rag\n",
    "from hybrid_database import hybrid_search\n",
    "from hybrid_database import data_preprocessing\n",
    "from FlagEmbedding import FlagLLMReranker\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef590b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(config):\n",
    "    \n",
    "    database, embedding_model = data_preprocessing(config)\n",
    "    print(\"loaded datbase and embedding_model\")\n",
    "\n",
    "    rerank_model = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_fp16=False, devices=config[\"device\"])\n",
    "    print(\"loaded rerank model\")\n",
    "\n",
    "    llm_model = ChatOpenAI(\n",
    "        model_name=\"Qwen/Qwen3-8B-AWQ\",\n",
    "        base_url=\"http://localhost:8000/v1\",\n",
    "        api_key=\"token-abc123\",\n",
    "        model_kwargs={\n",
    "            \"extra_body\": {\n",
    "                \"guided_decoding_backend\": \"xgrammar\" \n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    print(\"loaded llm model\")\n",
    "    print(\"All models loaded!!\")\n",
    "    return database, embedding_model, rerank_model, llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6344333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get models, embeddings and index\n",
    "config = config_rag()\n",
    "\n",
    "database, embedding_model, rerank_model, llm_model = get_models(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e0b0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Who is persona 5 main character?', 'response_time': 0.88, 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://en.wikipedia.org/wiki/Joker_(Persona)', 'title': 'Joker (Persona) - Wikipedia', 'content': '# Joker (*Persona*). **Ren Amamiya**, also better known by his codename **Joker**, is the main protagonist of *Persona 5*, a 2016 role-playing video game owned by Sega subsidiary Atlus. Ultimate* by Nintendo, and the kart racing game *Sonic Racing: CrossWorlds* by Sega and Sonic Team He was designed by *Persona* series artist Shigenori Soejima, and is voiced in Japanese by Jun Fukuyama and in English by Xander Mobus. Joker is a male teenage character created by artist Shigenori Soejima for the 2016 video game *Persona 5*. Joker\\'s initial Persona was originally the German demon Mephistopheles, but it was changed to Arsène since it better fit the game\\'s themes. Joker was introduced in *Persona 5* as the game\\'s main playable character and has appeared in the spin-off games *Persona 5: Dancing in Starlight*, *Persona Q2: New Cinema Labyrinth*, *Persona 5 Strikers*, *Persona 5 Tactica*, and *Persona 5: The Phantom X*. \"Persona 5\\'s Joker To Join Super Smash Bros.', 'score': 0.90830135, 'raw_content': None}, {'url': 'https://www.g2a.com/news/features/persona-5-characters-list-meet-the-phantom-thieves/', 'title': 'Persona 5 Characters List: Meet the Phantom Thieves - G2A News', 'content': 'The protagonist of Persona 5 – Joker – wouldn’t go far without the support of his fellow Phantom Thieves of Hearts. Joker is the protagonist of Persona 5 and Persona 5 Strikers, the leader of the Phantom Thieves of Hearts. Morgana awakens to his initial persona – Zorro – long before the other Phantom Thieves. Only when the Phantom Thieves drag him into the Metaverse and make him realise that Madarame sees him as nothing more than a slave does Yusuke finally stop defending his mentor – and he awakens to his persona, Goemon. When Joker and most of the other teammates meet her, she has already awakened to her Persona and explored her father’s Palace with the help of Morgana who, at the time, worked separately from the rest of the Phantom Thieves. When he comes across her Shadow in her Jail, Zenkichi – now an official member of the Phantom Thieves – awakens to his persona, Valjean.', 'score': 0.8674071, 'raw_content': None}, {'url': 'https://hero.fandom.com/wiki/Ren_Amamiya', 'title': 'Ren Amamiya | Heroes Wiki | Fandom', 'content': 'Ren Amamiya, better known by his code name Joker or The Phantom, is the main protagonist of the Persona 5 sub-series within the Persona series.', 'score': 0.8215175, 'raw_content': None}, {'url': 'https://horrorflora.com/2021/10/31/fan-ichf-akira-kurusu-aka-ren-amamiya-aka-joker-aka-persona-5-protagonist/', 'title': 'Fan ICHF: Akira Kurusu aka Ren Amamiya aka Joker aka Persona 5 ...', 'content': 'Like more than a few other people, I was introduced to\\xa0*Persona*\\xa0thanks to a well-known publicity stunt in the 2018 Game Awards that announced to the world at large that the protagonist of\\xa0*Persona 5*, codenamed Joker, was being added as a playable fighter to\\xa0*Super Smash Bros. Once they reform Kamoshida by stealing his “heart” (the most prized artifact within their personal “Palace” within the Metaverse representing their sinful behavior) and witness his public confession, the group become the founding members of the Phantom Thieves of Hearts, and end up getting involved in more adventures of escalating urgency and recruiting more and more members until, surprise surprise, they uncover a local governmental conspiracy and are forced to steal the “heart” of the very same person who framed Joker in the first place, the corrupt politician at the head of said conspiracy. Joker and his friends, on the other hand, get Personas themed after famous rebels, outlaws, and shady but ultimately well-intentioned characters across history and fiction.', 'score': 0.7969415, 'raw_content': None}, {'url': 'https://megamitensei.fandom.com/wiki/Ren_Amamiya', 'title': 'Ren Amamiya | Megami Tensei Wiki - Fandom', 'content': 'The protagonist of Persona 5, canonically named Ren Amamiya, is a second-year transfer student at Shujin Academy, being placed there to continue his academics', 'score': 0.76554126, 'raw_content': None}], 'request_id': 'eb3fab81-ba80-4aa8-b978-f4bee8ae7068'}\n"
     ]
    }
   ],
   "source": [
    "# tavilly client\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = config[\"tavilly_api_key\"]\n",
    "\n",
    "web_tool = TavilySearch(max_results=5, topic=\"general\", include_images=False, )\n",
    "docs = web_tool.invoke({\"query\": \"Who is persona 5 main character?\"})\n",
    "\n",
    "print(docs)\n",
    "web_results = \"\\n\".join([d[\"content\"] for d in docs[\"results\"]])\n",
    "print(web_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca70d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tools\n",
    "\n",
    "\n",
    "# # Augment the LLM with tools\n",
    "# tools = []\n",
    "# tools_by_name = {tool.name: tool for tool in tools}\n",
    "# model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c371954",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\n",
    "documents = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba8a7d",
   "metadata": {},
   "source": [
    "### LLMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d05c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# router node\n",
    "class RouteDecision(BaseModel):\n",
    "    \"\"\"Routes the user query to the appropriate data source.\"\"\"\n",
    "    reasoning: str = Field(\n",
    "        ..., \n",
    "        description=\"Briefly explain what the user is asking and check if it matches the explicit database topics.\"\n",
    "    )\n",
    "    is_in_domain: bool = Field(\n",
    "        ..., \n",
    "        description=\"True ONLY if the query is strictly about the specific topics in our database (e.g., vLLM, CRAG, Self-RAG). False for general ML topics like Federated Learning, Vision, etc.\"\n",
    "    )\n",
    "    route: Literal[\"vectorstore\", \"websearch\", \"chitchat\"] = Field(\n",
    "        ..., \n",
    "        description=\"Choose 'vectorstore' if is_in_domain is True. Choose 'websearch' if is_in_domain is False or for current events. Choose 'chitchat' for greetings.\"\n",
    "    )\n",
    "\n",
    "router_node_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are an expert routing assistant for a highly specialized AI Engineering knowledge base. \\nYour job is to analyze the user\\'s query and route it to the correct destination.\\n\\nCRITICAL CONTEXT: Our \\'vectorstore\\' is NOT a general Machine Learning database. It ONLY contains 15 specific research papers. \\nThe explicit topics covered in the vectorstore are:\\n- LLM Architecture & Serving (vLLM, Transformers, PagedAttention)\\n- Advanced RAG Methodologies (CRAG, Self-RAG, Adaptive-RAG, Vector Databases)\\n- Model Building from Scratch (ConvNeXt)\\n\\nRouting Rules:\\n1. Route to \\'vectorstore\\' ONLY if the query is explicitly related to the specific topics/papers listed above.\\n2. Route to \\'websearch\\' if the user asks about General ML/AI topics NOT listed above (e.g., Federated Learning, Reinforcement Learning, CNNs, Generative Adversarial Networks).\\n3. Route to \\'websearch\\' if the user asks about real-world current events, news, or live data.\\n4. Route to \\'chitchat\\' if the query is a simple conversational greeting or compliment.\"),\n",
    "        (\"human\", \"Route this query: {question}\"),\n",
    "    ],\n",
    "    input_variables = [\"question\"],\n",
    ")\n",
    "\n",
    "router_node_llm = router_node_prompt | llm_model.with_structured_output(RouteDecision, method=\"json_schema\", strict=True)\n",
    "\n",
    "result = router_node_llm.invoke({\"question\": question})\n",
    "\n",
    "print(type(result))\n",
    "print(result.route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite node\n",
    "class RewrittenQuery(BaseModel):\n",
    "    \"\"\"The optimized search query for the vector database.\"\"\"\n",
    "    reasoning: str = Field(..., description=\"Briefly explain what keywords were extracted or expanded.\")\n",
    "    query: str = Field(..., description=\"The highly optimized, keyword-dense search query.\")\n",
    "\n",
    "\n",
    "rewrite_node_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are an expert Search Query Optimizer. \\nThe user previously searched a vector database, but the retrieval failed to find relevant context.\\n\\nYour task is to rewrite the query to be highly effective for semantic search.\\nStrip away conversational filler (e.g., \\\"Can you tell me about...\\\"). Extract core technical keywords and expand known AI acronyms to maximize the chance of a database match.\"),\n",
    "        (\"human\", \"Original failing query: {question}\"),\n",
    "    ],\n",
    "    input_variables = [\"question\"],\n",
    ")\n",
    "\n",
    "rewrite_node_llm = rewrite_node_prompt | llm_model.with_structured_output(RewrittenQuery, method=\"json_schema\", strict=True)\n",
    "\n",
    "\n",
    "result = rewrite_node_llm.invoke({\"question\": question})\n",
    "\n",
    "print(type(result))\n",
    "print(result.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context checking before generation\n",
    "class ContextGap(BaseModel):\n",
    "    \"\"\"Evaluates if there is missing foundational knowledge in the retrieved documents.\"\"\"\n",
    "    reasoning: str = Field(..., description=\"Explain if any concepts mentioned in the documents are required to answer the prompt, but are not defined.\")\n",
    "    has_gap: bool = Field(..., description=\"True if a critical concept/acronym is missing its definition. False if the context is complete.\")\n",
    "    missing_concept_query: str = Field(default=\"\", description=\"If has_gap is True, provide a 2-4 word search query to find the missing concept. If False, leave empty.\")\n",
    "\n",
    "context_check_node_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a Context Gap Analyzer. \\nYour job is to read the user\\'s question and the currently retrieved documents to determine if a \\\"Multi-Hop\\\" search is required.\\n\\nLook for unexplained technical acronyms or foundational concepts that are mentioned in the text as part of the answer, but are not actually explained. \\nIf explaining that concept is necessary to fully answer the user\\'s question, flag a context gap and provide a short search query to retrieve that specific definition.\"),\n",
    "        (\"human\", \"User Question: {question}\\n\\nCurrently Retrieved Documents:\\n{documents}\"),\n",
    "    ],\n",
    "    input_variables = [\"question\", \"documents\"],\n",
    ")\n",
    "\n",
    "context_check_node_llm = context_check_node_prompt | llm_model.with_structured_output(ContextGap, method=\"json_schema\", strict=True)\n",
    "\n",
    "result = context_check_node_llm.invoke({\"question\": question, \"documents\": documents})\n",
    "\n",
    "print(type(result))\n",
    "print(result.has_gap)\n",
    "print(result.missing_concept_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate node\n",
    "generate_node_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a highly technical AI Research Assistant.\\nAnswer the user\\'s question strictly using the provided context documents.\\n\\nRules:\\n1. If the answer is not contained within the context documents, state: \\\"I cannot answer this based on the provided context.\\\"\\n2. Do not use outside knowledge or hallucinate facts.\\n3. Keep the answer concise, professional, and directly address the user\\'s core question.\\n4. When applicable, briefly cite the document or paper title you are referencing.\"),\n",
    "        (\"human\", \"Context Documents:\\n{documents}\\n\\n---------------------\\nUser Question: {question}\"),\n",
    "    ],\n",
    "    input_variables = [\"documents\", \"question\"],\n",
    ")\n",
    "\n",
    "generate_node_llm = generate_node_prompt | llm_model | StrOutputParser()\n",
    "\n",
    "result = generate_node_llm.invoke({\"question\": question, \"documents\": documents})\n",
    "\n",
    "print(result)\n",
    "generation = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ccfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hallucination checker\n",
    "class HallucinationScore(BaseModel):\n",
    "    \"\"\"Checks if the generated answer contains hallucinations.\"\"\"\n",
    "    reasoning: str = Field(..., description=\"Briefly compare the generated facts against the source documents.\")\n",
    "    is_grounded: bool = Field(..., description=\"True if all facts in the generation are present in the documents. False if any outside info was added.\")\n",
    "\n",
    "hallucination_check_node_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a strict grading assistant executing a Self-RAG critique.\\nYour task is to evaluate if an AI-generated answer is completely grounded in the provided source documents.\\n\\nA generation is ungrounded (hallucinated) if it includes statistics, methodology names, or factual claims that do not appear anywhere in the source text.\"),\n",
    "        (\"human\", \"Source Documents:\\n{documents}\\n\\n---------------------\\nAI Generation:\\n{generation}\"),\n",
    "    ],\n",
    "    input_variables = [\"documents\", \"generation\"],\n",
    ")\n",
    "\n",
    "hallucination_check_node_llm = hallucination_check_node_prompt | llm_model.with_structured_output(HallucinationScore, method=\"json_schema\", strict=True)\n",
    "\n",
    "result = hallucination_check_node_llm.invoke({\"documents\": documents, \"generation\": generation})\n",
    "\n",
    "print(type(result))\n",
    "print(result.is_grounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb44acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevance checker\n",
    "class RelevanceScore(BaseModel):\n",
    "    \"\"\"Checks if the generated answer actually addresses the user's question.\"\"\"\n",
    "    reasoning: str = Field(..., description=\"Explain how the generated text does or does not answer the core premise of the user's question.\")\n",
    "    is_relevant: bool = Field(..., description=\"True if the answer directly addresses the question. False if it is evasive or off-topic.\")\n",
    "\n",
    "relevance_check_node_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a strict grading assistant. \\nYour task is to evaluate if an AI-generated answer successfully and directly addresses the user\\'s original question.\\n\\nFail the generation if it just summarizes the documents without answering the specific user prompt, or if it is overly evasive.\"),\n",
    "        (\"human\", \"User Question: {question}\\n\\n---------------------\\nAI Generation: {generation}\"),\n",
    "    ],\n",
    "    input_variables = [\"question\", \"generation\"],\n",
    ")\n",
    "\n",
    "relevance_check_node_llm = relevance_check_node_prompt | llm_model.with_structured_output(RelevanceScore, method=\"json_schema\", strict=True)\n",
    "\n",
    "result = relevance_check_node_llm.invoke({\"question\": question, \"generation\": generation})\n",
    "\n",
    "print(type(result))\n",
    "print(result.is_relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph state\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        loop_count: number to track loops\n",
    "        gen_retries: number of generates for hallucinations\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: list\n",
    "    missing_concept_query: str\n",
    "    loop_count: int\n",
    "    gen_retries: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb30471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "def retrieve_and_rerank(state):\n",
    "    \"\"\"\n",
    "    Retrieves documents from the index and immediately reranks them, \n",
    "    applying a score filter and keeping only the top k.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state containing strictly the highly relevant documents.\n",
    "    \"\"\"\n",
    "    print(\"\\n---RETRIEVE & RERANK---\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    loop_count = state.get(\"loop_count\", 0)\n",
    "\n",
    "    print(f\"Retrieving for question: {question}\")\n",
    "    raw_docs = hybrid_search(database, embedding_model, question, sparse_weight=0.7, dense_weight=1, limit=20)\n",
    "\n",
    "    if not raw_docs:\n",
    "        print(\"No documents found in database.\")\n",
    "        return {\"documents\": [], \"question\": question, \"loop_count\": loop_count}\n",
    "\n",
    "    print(\"Reranking retrieved documents...\")\n",
    "    question_and_docs = [[question, doc[\"text\"]] for doc in raw_docs]\n",
    "    scores = rerank_model.compute_score(question_and_docs, normalize=True, batch_size=4, max_length=1024)\n",
    "    \n",
    "    filtered_pairs = [(doc, score) for doc, score in zip(raw_docs, scores) if score > 0.5]\n",
    "    reranked_docs = heapq.nlargest(5, filtered_pairs, key=lambda x: x[1])\n",
    "    \n",
    "    final_documents = [doc for doc, score in reranked_docs]\n",
    "    \n",
    "    print(f\"Milvus found {len(raw_docs)} docs. Reranker kept {len(final_documents)} docs > 0.5 score.\")\n",
    "    \n",
    "    return {\"documents\": final_documents, \"question\": question, \"loop_count\": loop_count}       \n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    generate answer for query\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    print(\"\\n---GENERATE---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    loop_count = state[\"loop_count\"]\n",
    "    gen_retries = state.get(\"gen_retries\", 0)\n",
    "\n",
    "    result = generate_node_llm.invoke({\"question\": question, \"documents\": documents})\n",
    "    generation = result\n",
    "\n",
    "    gen_retries += 1\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation, \"loop_count\": loop_count, \"gen_retries\": gen_retries}\n",
    "\n",
    "def rewrite_query(state):\n",
    "    \"\"\"\n",
    "    rewrite query for better question\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    print(\"\\n---REWRITE_QUERY---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    loop_count = state[\"loop_count\"]\n",
    "    loop_count += 1\n",
    "\n",
    "    result = rewrite_node_llm.invoke({\"question\": question})\n",
    "    rewritten_query = result.query\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": rewritten_query, \"loop_count\": loop_count}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    web search for query to find relevant context\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    print(\"\\n---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    docs = web_tool.invoke({\"query\": f\"{question}\"})\n",
    "\n",
    "    print(docs)\n",
    "    web_results = [d[\"content\"] for d in docs[\"results\"]]\n",
    "    print(web_results)\n",
    "    \n",
    "    return {\"documents\": web_results, \"question\": question}\n",
    "\n",
    "def chitchat_node(state):\n",
    "    \"\"\"Handles simple greetings without searching the database.\"\"\"\n",
    "    print(\"\\n---CHITCHAT---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    result = generate_node_llm.invoke(f\"The user said: '{question}'. Respond politely and concisely.\")\n",
    "    \n",
    "    return {\"generation\": result, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edges\n",
    "\n",
    "def query_router(state):\n",
    "    \"\"\"\n",
    "    [Adaptive RAG]: routes the query to vectorbase or simple llm call or more\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    print(\"\\n---Query Router--\")\n",
    "    question = state[\"question\"]\n",
    "    result = router_node_llm.invoke({\"question\": question})\n",
    "    print(result.reasoning)\n",
    "    if result.route == \"vectorstore\":\n",
    "        print(\"---Route question to RAG\")\n",
    "        return \"vectorstore\"\n",
    "    elif result.route == \"websearch\":\n",
    "        print(\"---Route question to Websearch\")\n",
    "        return \"websearch\"\n",
    "    elif result.route == \"chitchat\":\n",
    "        print(\"---Route question to casual LLM\")\n",
    "        return \"chitchat\"\n",
    "\n",
    "def rewrite_router(state):\n",
    "    \"\"\"\n",
    "    [Adaptive RAG]: routes the query to vectorbase or simple llm call or more\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    loop_count = state[\"loop_count\"]\n",
    "    if loop_count > 3:\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        return \"retrieve\"\n",
    "\n",
    "def hallucinations_and_relevence_router(state):\n",
    "    \"\"\"\n",
    "    [Adaptive RAG]: routes the query to vectorbase or simple llm call or more\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    gen_retries = state[\"gen_retries\"]\n",
    "\n",
    "    # hallucination check\n",
    "    print(\"\\n---Checking hallucinations---\")\n",
    "    result = hallucination_check_node_llm.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    print(result.reasoning)\n",
    "    if result.is_grounded:\n",
    "        print(\"Hallucination Check passed\")\n",
    "        print(\"\\n---Check relevance---\")\n",
    "        result = relevance_check_node_llm.invoke({\"question\": question, \"generation\": generation})\n",
    "        print(result.reasoning)\n",
    "        if result.is_relevant:\n",
    "            return \"all_pass\"\n",
    "    \n",
    "    if gen_retries >= 2:\n",
    "        return \"rewrite_query\"\n",
    "    else:\n",
    "        return \"generate\"\n",
    "    \n",
    "def post_retrieve_router(state):\n",
    "    \"\"\"\n",
    "    Checks if any documents survived the reranker cutoff.\n",
    "    \"\"\"\n",
    "    print(\"\\n---POST RETRIEVE ROUTER---\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "    \n",
    "    if len(documents) == 0:\n",
    "        print(\"---No relevant documents found. Routing to REWRITE---\")\n",
    "        return \"rewrite\"\n",
    "    else:\n",
    "        print(f\"---Found {len(documents)} relevant documents. Routing to GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aa3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 2. Add all the Nodes (The Workers)\n",
    "workflow.add_node(\"retrieve_and_rerank\", retrieve_and_rerank)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"rewrite_query\", rewrite_query)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"chitchat\", chitchat_node)\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    query_router,\n",
    "    {\n",
    "        \"vectorstore\": \"retrieve_and_rerank\",\n",
    "        \"websearch\": \"web_search\",\n",
    "        \"chitchat\": \"chitchat\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve_and_rerank\",\n",
    "    post_retrieve_router,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"rewrite\": \"rewrite_query\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"rewrite_query\",\n",
    "    rewrite_router,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve_and_rerank\",\n",
    "        \"web_search\": \"web_search\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    hallucinations_and_relevence_router,\n",
    "    {\n",
    "        \"all_pass\": END,\n",
    "        \"generate\": \"generate\",          \n",
    "        \"rewrite_query\": \"rewrite_query\" \n",
    "    }\n",
    ")\n",
    "\n",
    "# Chitchat just ends the graph\n",
    "workflow.add_edge(\"chitchat\", END)\n",
    "\n",
    "# 4. Compile into a runnable application\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"Graph compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a0581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"What is the AlphaCodium paper about?\"}\n",
    "\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
