{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from langchain.messages import AnyMessage\n",
    "from typing_extensions import TypedDict, List, Annotated, Optional, Literal\n",
    "from langchain.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import operator\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import heapq\n",
    "\n",
    "from helper import get_models\n",
    "from hybrid_database import hybrid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6344333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get models, embeddings and index\n",
    "database, embedding_model, rerank_model, llm_model = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca70d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tools\n",
    "\n",
    "\n",
    "# # Augment the LLM with tools\n",
    "# tools = []\n",
    "# tools_by_name = {tool.name: tool for tool in tools}\n",
    "# model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c371954",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\n",
    "documents = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba8a7d",
   "metadata": {},
   "source": [
    "### LLMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d05c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# router node\n",
    "class RouteDecision(BaseModel):\n",
    "    \"\"\"Routes the user query to the appropriate data source.\"\"\"\n",
    "    reasoning: str = Field(\n",
    "        ..., \n",
    "        description=\"Briefly explain what the user is asking and check if it matches the explicit database topics.\"\n",
    "    )\n",
    "    is_in_domain: bool = Field(\n",
    "        ..., \n",
    "        description=\"True ONLY if the query is strictly about the specific topics in our database (e.g., vLLM, CRAG, Self-RAG). False for general ML topics like Federated Learning, Vision, etc.\"\n",
    "    )\n",
    "    route: Literal[\"vectorstore\", \"websearch\", \"chitchat\"] = Field(\n",
    "        ..., \n",
    "        description=\"Choose 'vectorstore' if is_in_domain is True. Choose 'websearch' if is_in_domain is False or for current events. Choose 'chitchat' for greetings.\"\n",
    "    )\n",
    "\n",
    "router_node_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are an expert routing assistant for a highly specialized AI Engineering knowledge base. \\nYour job is to analyze the user\\'s query and route it to the correct destination.\\n\\nCRITICAL CONTEXT: Our \\'vectorstore\\' is NOT a general Machine Learning database. It ONLY contains 15 specific research papers. \\nThe explicit topics covered in the vectorstore are:\\n- LLM Architecture & Serving (vLLM, Transformers, PagedAttention)\\n- Advanced RAG Methodologies (CRAG, Self-RAG, Adaptive-RAG, Vector Databases)\\n- Model Building from Scratch (ConvNeXt)\\n\\nRouting Rules:\\n1. Route to \\'vectorstore\\' ONLY if the query is explicitly related to the specific topics/papers listed above.\\n2. Route to \\'websearch\\' if the user asks about General ML/AI topics NOT listed above (e.g., Federated Learning, Reinforcement Learning, CNNs, Generative Adversarial Networks).\\n3. Route to \\'websearch\\' if the user asks about real-world current events, news, or live data.\\n4. Route to \\'chitchat\\' if the query is a simple conversational greeting or compliment.\"),\n",
    "        (\"human\", \"Route this query: {question}\"),\n",
    "    ],\n",
    "    input_variables = [\"question\"],\n",
    ")\n",
    "\n",
    "router_node_llm = router_node_prompt | llm_model.with_structured_output(RouteDecision, method=\"json_schema\", strict=True)\n",
    "\n",
    "result = router_node_llm.invoke({\"question\": question})\n",
    "\n",
    "print(type(result))\n",
    "print(result.route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite node\n",
    "class RewrittenQuery(BaseModel):\n",
    "    \"\"\"The optimized search query for the vector database.\"\"\"\n",
    "    reasoning: str = Field(..., description=\"Briefly explain what keywords were extracted or expanded.\")\n",
    "    query: str = Field(..., description=\"The highly optimized, keyword-dense search query.\")\n",
    "\n",
    "\n",
    "rewrite_node_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are an expert Search Query Optimizer. \\nThe user previously searched a vector database, but the retrieval failed to find relevant context.\\n\\nYour task is to rewrite the query to be highly effective for semantic search.\\nStrip away conversational filler (e.g., \\\"Can you tell me about...\\\"). Extract core technical keywords and expand known AI acronyms to maximize the chance of a database match.\"),\n",
    "        (\"human\", \"Original failing query: {question}\"),\n",
    "    ],\n",
    "    input_variables = [\"question\"],\n",
    ")\n",
    "\n",
    "rewrite_node_llm = rewrite_node_prompt | llm_model.with_structured_output(RewrittenQuery, method=\"json_schema\", strict=True)\n",
    "\n",
    "\n",
    "result = rewrite_node_llm.invoke({\"question\": question})\n",
    "\n",
    "print(type(result))\n",
    "print(result.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context checking before generation\n",
    "class ContextGap(BaseModel):\n",
    "    \"\"\"Evaluates if there is missing foundational knowledge in the retrieved documents.\"\"\"\n",
    "    reasoning: str = Field(..., description=\"Explain if any concepts mentioned in the documents are required to answer the prompt, but are not defined.\")\n",
    "    has_gap: bool = Field(..., description=\"True if a critical concept/acronym is missing its definition. False if the context is complete.\")\n",
    "    missing_concept: str = Field(default=\"\", description=\"If has_gap is True, provide a 2-4 word search query to find the missing concept. If False, leave empty.\")\n",
    "\n",
    "\n",
    "sys_msg = SystemMessage(content=\"\")\n",
    "user_msg = HumanMessage(content=f\"\")\n",
    "\n",
    "context_check_node_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a Context Gap Analyzer. \\nYour job is to read the user\\'s question and the currently retrieved documents to determine if a \\\"Multi-Hop\\\" search is required.\\n\\nLook for unexplained technical acronyms or foundational concepts that are mentioned in the text as part of the answer, but are not actually explained. \\nIf explaining that concept is necessary to fully answer the user\\'s question, flag a context gap and provide a short search query to retrieve that specific definition.\"),\n",
    "        (\"human\", \"User Question: {question}\\n\\nCurrently Retrieved Documents:\\n{documents}\"),\n",
    "    ],\n",
    "    input_variables = [\"question\", \"documents\"],\n",
    ")\n",
    "\n",
    "context_check_node_llm = context_check_node_prompt | llm_model.with_structured_output(ContextGap, method=\"json_schema\", strict=True)\n",
    "\n",
    "result = context_check_node_llm.invoke({\"question\": question, \"documents\": documents})\n",
    "\n",
    "print(type(result))\n",
    "print(result.has_gap)\n",
    "print(result.missing_concept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate node\n",
    "generate_node_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a highly technical AI Research Assistant.\\nAnswer the user\\'s question strictly using the provided context documents.\\n\\nRules:\\n1. If the answer is not contained within the context documents, state: \\\"I cannot answer this based on the provided context.\\\"\\n2. Do not use outside knowledge or hallucinate facts.\\n3. Keep the answer concise, professional, and directly address the user\\'s core question.\\n4. When applicable, briefly cite the document or paper title you are referencing.\"),\n",
    "        (\"human\", \"Context Documents:\\n{documents}\\n\\n---------------------\\nUser Question: {question}\"),\n",
    "    ],\n",
    "    input_variables = [\"documents\", \"question\"],\n",
    ")\n",
    "\n",
    "generate_node_llm = generate_node_prompt | llm_model | StrOutputParser()\n",
    "\n",
    "result = generate_node_llm.invoke({\"question\": question, \"documents\": documents})\n",
    "\n",
    "print(result)\n",
    "generation = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ccfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hallucination checker\n",
    "class HallucinationScore(BaseModel):\n",
    "    \"\"\"Checks if the generated answer contains hallucinations.\"\"\"\n",
    "    reasoning: str = Field(..., description=\"Briefly compare the generated facts against the source documents.\")\n",
    "    is_grounded: bool = Field(..., description=\"True if all facts in the generation are present in the documents. False if any outside info was added.\")\n",
    "\n",
    "hallucination_check_node_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a strict grading assistant executing a Self-RAG critique.\\nYour task is to evaluate if an AI-generated answer is completely grounded in the provided source documents.\\n\\nA generation is ungrounded (hallucinated) if it includes statistics, methodology names, or factual claims that do not appear anywhere in the source text.\"),\n",
    "        (\"human\", \"Source Documents:\\n{documents}\\n\\n---------------------\\nAI Generation:\\n{generation}\"),\n",
    "    ],\n",
    "    input_variables = [\"documents\", \"generation\"],\n",
    ")\n",
    "\n",
    "hallucination_check_node_llm = hallucination_check_node_prompt | llm_model.with_structured_output(HallucinationScore, method=\"json_schema\", strict=True)\n",
    "\n",
    "result = hallucination_check_node_llm.invoke({\"documents\": documents, \"generation\": generation})\n",
    "\n",
    "print(type(result))\n",
    "print(result.is_grounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb44acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevance checker\n",
    "class RelevanceScore(BaseModel):\n",
    "    \"\"\"Checks if the generated answer actually addresses the user's question.\"\"\"\n",
    "    reasoning: str = Field(..., description=\"Explain how the generated text does or does not answer the core premise of the user's question.\")\n",
    "    is_relevant: bool = Field(..., description=\"True if the answer directly addresses the question. False if it is evasive or off-topic.\")\n",
    "\n",
    "relevance_check_node_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a strict grading assistant. \\nYour task is to evaluate if an AI-generated answer successfully and directly addresses the user\\'s original question.\\n\\nFail the generation if it just summarizes the documents without answering the specific user prompt, or if it is overly evasive.\"),\n",
    "        (\"human\", \"User Question: {question}\\n\\n---------------------\\nAI Generation: {generation}\"),\n",
    "    ],\n",
    "    input_variables = [\"question\", \"generation\"],\n",
    ")\n",
    "\n",
    "relevance_check_node_llm = relevance_check_node_prompt | llm_model.with_structured_output(RelevanceScore, method=\"json_schema\", strict=True)\n",
    "\n",
    "result = relevance_check_node_llm.invoke({\"question\": question, \"generation\": generation})\n",
    "\n",
    "print(type(result))\n",
    "print(result.is_relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph state\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        loop_count: number to track loops\n",
    "        gen_retries: number of generates for hallucinations\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: Annotated[list, operator.add]\n",
    "    missing_concepts: str\n",
    "    loop_count: int\n",
    "    gen_retries: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb30471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    retrieve documents from the index\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    loop_count = state[\"loop_count\"]\n",
    "\n",
    "    loop_count += 1\n",
    "    print(f\"incrementing loop_count: {loop_count}\\n\")\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Documents: {documents}\")\n",
    "    documents = hybrid_search(database, embedding_model, question, sparse_weight=0.7, dense_weight=1, limit=20)\n",
    "    return {\"documents\": documents, \"question\": question, \"loop_count\": loop_count}\n",
    "\n",
    "def rerank_documents(state):\n",
    "    \"\"\"\n",
    "    rerank the documents using the rerank model, apply a filter for scores and keeps only the top k\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): reranks documents and select top k documents\n",
    "    \"\"\"\n",
    "    print(\"\\n---RERANK---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    docs = state[\"documents\"]\n",
    "    loop_count = state[\"loop_count\"]\n",
    "\n",
    "    question_and_docs = [[question, doc[\"text\"]] for doc in docs]\n",
    "    scores = rerank_model.compute_score(question_and_docs, normalize=True)\n",
    "    filtered_pairs = ((doc, score) for doc, score in zip(docs, scores) if score > 0.5)  # threshold for filter 0.5\n",
    "    reranked_docs = heapq.nlargest(5, filtered_pairs, key=lambda x: x[1])\n",
    "    documents = [doc for doc, score in reranked_docs]\n",
    "    print(f\"Reranked document: {documents}\")\n",
    "    return {\"documents\": documents, \"question\": question, \"loop_count\": loop_count}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    generate answer for query\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    print(\"\\n---GENERATE---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    loop_count = state[\"loop_count\"]\n",
    "\n",
    "    prompt = {\n",
    "        \"system\": \"\",\n",
    "        \"user\"  : f\"\"\n",
    "    }\n",
    "    completion = llm_model.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-8B-AWQ\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": prompt[\"user\"]},\n",
    "        ],\n",
    "    )\n",
    "    generation = completion.choices[0].message\n",
    "\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation, \"loop_count\": loop_count}\n",
    "\n",
    "def rewrite_query(state):\n",
    "    \"\"\"\n",
    "    rewrite query for better question\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    print(\"\\n---REWRITE_QUERY---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    loop_count = state[\"loop_count\"]\n",
    "\n",
    "    prompt = {\n",
    "        \"system\": \"\",\n",
    "        \"user\"  : \"\"\n",
    "    }\n",
    "    completion = llm_model.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-8B-AWQ\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": prompt[\"user\"]},\n",
    "        ],\n",
    "    )\n",
    "    rewritten_query = completion.choices[0].message\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": rewritten_query, \"loop_count\": loop_count}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    web search for query to find relevant context\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edges\n",
    "\n",
    "def query_router(state):\n",
    "    \"\"\"\n",
    "    [Adaptive RAG]: routes the query to vectorbase or simple llm call or more\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    result = router_node_llm.invoke({\"question\": question})\n",
    "    \n",
    "    return state\n",
    "\n",
    "def check_context(state):\n",
    "    \"\"\"\n",
    "    check relevance before generation, else retrieve or get more info about missing elements\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): reranks documents and select top k documents\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    loop_count = state[\"loop_count\"]\n",
    "    gen_retries = state.get(\"gen_retries\", 0)\n",
    "\n",
    "    print(\"\\n---CHECKING CONTEXT OF DOCUMENTS---\")\n",
    "    \n",
    "    return \n",
    "\n",
    "def rerank_router(state):\n",
    "    \"\"\"\n",
    "    [Adaptive RAG]: routes the query to vectorbase or simple llm call or more\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "def rewrite_router(state):\n",
    "    \"\"\"\n",
    "    [Adaptive RAG]: routes the query to vectorbase or simple llm call or more\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "def hallucinations_and_relevence_router(state):\n",
    "    \"\"\"\n",
    "    [Adaptive RAG]: routes the query to vectorbase or simple llm call or more\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): retrieved documents added to the state\n",
    "    \"\"\"\n",
    "    \n",
    "    return state"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
